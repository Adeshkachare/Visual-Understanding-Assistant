
# ğŸ“¹ Visual Understanding Chat Assistant

A real-time visual understanding system that analyzes short videos (â‰¤2 mins), detects events and violations using object detection (YOLOv5), summarizes key incidents, and enables conversational interaction with the video using Mistral-7B-based LLM.

![Demo](https://youtu.be/Y1QyB7cpzc0)

---

## ğŸš€ Features

- ğŸ¥ **Video Upload** â€“ Supports `.mp4`, `.avi`, `.mov` formats up to 2 minutes.
- ğŸ§  **Event Detection** â€“ Detects crowding or custom events using YOLOv5n.
- ğŸ“ **Summarization** â€“ Generates clear, structured summaries of detected events.
- ğŸ’¬ **Conversational AI** â€“ Ask questions about the video and get relevant responses.
- ğŸ–¼ï¸ **Snapshots** â€“ Shows event-based video snapshots with timestamps.
- ğŸ” **Semantic Search + RAG** â€“ Uses HuggingFace embeddings and FAISS for contextual retrieval.
- âš¡ **Fast Inference** â€“ Uses the `mistralai/Mistral-7B-Instruct-v0.2` model with GPU acceleration.
- ğŸ›ï¸ **Streamlit UI** â€“ Interactive, fast, and intuitive frontend.

---

## ğŸ› ï¸ Tech Stack

| Component              | Technology                                  |
|------------------------|---------------------------------------------|
| UI/Frontend            | Streamlit                                   |
| Object Detection       | YOLOv5n via Torch Hub                        |
| Embedding Model        | `all-MiniLM-L6-v2` (SentenceTransformer)     |
| Language Model (LLM)   | `mistralai/Mistral-7B-Instruct-v0.2`         |
| Text Chunking          | LangChain `RecursiveCharacterTextSplitter`  |
| Vector DB              | FAISS (in-memory)                           |
| Chat Memory            | LangChain `ConversationBufferMemory`        |
| Video Processing       | OpenCV + ffmpeg (via subprocess)            |

---

## âš™ï¸ Setup Instructions

### 1. Clone Repository
```bash
git clone https://github.com/your_username/visual-understanding-chat-assistant.git
cd visual-understanding-chat-assistant
```

### 2. Create Environment & Install Dependencies
```bash
pip install -r requirements.txt
```

#### Example `requirements.txt`:
```txt
streamlit
torch
opencv-python
transformers
sentence-transformers
langchain
faiss-cpu
```

> âš ï¸ You must have `ffmpeg` and `ffprobe` installed and available on your system. Update the `ffprobe_path` in `main_app.py` accordingly.

### 3. Run the App
```bash
streamlit run main_app.py
```

---

## ğŸ“‚ Project Structure

```
â”œâ”€â”€ main_app.py                # Main Streamlit application
â”œâ”€â”€ README.md                  # Project overview
â”œâ”€â”€ requirements.txt           # Required Python packages
â”œâ”€â”€ /ffmpeg-bin/               # ffprobe & ffmpeg executables
â”œâ”€â”€ /snapshots/ (optional)     # Saved frames of detected events
```

---

## ğŸ§ª How It Works

1. **Upload** a video file (max 2 mins).
2. **Extract Frames** at 1 fps for efficient processing.
3. **Detect Objects** in each frame using YOLOv5n.
4. **Flag Events** such as crowding (>3 people).
5. **Summarize Events** as text.
6. **Embed + Store** summaries in a vector DB.
7. **Chat** with a retrieval-augmented Mistral-7B chatbot about the video.

---

## ğŸ¤– Example Questions to Ask the Assistant

- *"What happened at the 1-minute mark?"*
- *"Were there any large crowds?"*
- *"Give a brief overview of the video."*
- *"How many people were detected overall?"*

---

## ğŸ§  Model Info

- ğŸ”¹ Mistral-7B: Efficient instruction-tuned model running with GPU acceleration.
- ğŸ”¹ Embeddings: `all-MiniLM-L6-v2` (lightweight and fast).
- ğŸ”¹ Detection: `yolov5n` for fast frame-level inference.

---

## ğŸ§© Limitations

- Currently supports basic event detection (e.g., crowding).
- LLM is locally hosted and requires sufficient VRAM.
- Video length is capped at 2 minutes for latency and performance.
- Doesnâ€™t support multi-class rule violations yet.

---

## ğŸ“Œ To-Do / Improvements

- [ ] Add custom violation logic (e.g., helmet detection, red light).
- [ ] Support for longer video processing with async inference.
- [ ] Integrate with real-time streams (e.g., RTSP, webcam).
- [ ] Optimize memory & improve snapshot tagging.
- [ ] Deployment using Docker or HuggingFace Spaces.

---

## ğŸ’¡ Inspiration

Built as part of **Mantra Hackathon 2025** to explore real-time video understanding and multi-modal AI agents.

---

## ğŸ“¬ Contact

- **Developer**: Adesh Kachare  
- **Email**: your.email@example.com  
- **LinkedIn**: [linkedin.com/in/yourprofile](https://linkedin.com/in/yourprofile)  
- **GitHub**: [@yourusername](https://github.com/yourusername)

---

## ğŸ“ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
